{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jseabold/dask-pydata-chi-2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Me\n",
    "\n",
    "* Data Scientist at [Civis Analytics](https://civisanalytics.com/)\n",
    "* [@jseabold](http://twitter.com/jseabold)\n",
    "* <a href=\"mailto:jsseabold@gmail.com.com\" target=\"_top\">jsseabold@gmail.com</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"left\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* At a high-level, dask provides parallel NumPy and Pandas\n",
    "* It also provides primitives or ad-hoc parallel algorithms (dask.delayed similar to joblib.delayed)\n",
    "\n",
    "Dask is a flexible parallel computing library for analytics. Dask emphasizes the following virtues:\n",
    "\n",
    "* **Familiar**: Provides parallelized NumPy array and Pandas DataFrame objects\n",
    "* **Native**: Enables distributed computing in Pure Python with access to the PyData stack.\n",
    "* **Fast**: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms\n",
    "* **Flexible**: Supports complex and messy workloads\n",
    "* **Scales up**: Runs resiliently on clusters with 100s of nodes\n",
    "* **Scales down**: Trivial to set up and run on a laptop in a single process\n",
    "* **Responsive**: Designed with interactive computing in mind it provides rapid feedback and diagnostics to aid humans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Examples and Tutorials](http://dask.pydata.org/en/latest/examples-tutorials.html)\n",
    "\n",
    "[Matt Rocklin's Dask YouTube Playlist](https://www.youtube.com/channel/UCFYhuCL11p3oO9375_NbLQg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dask Computational Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parallel programming with task scheduling\n",
    "* Familiar abstractions for executing tasks in parallel on data that doesn't fit into memory\n",
    "  * Arrays, DataFrames\n",
    "* Task graphs\n",
    "  * Representation of a parallel computation\n",
    "* Scheduling\n",
    "  * Executes task graphs in parallel on a single machine using threads or processes\n",
    "  * Support for distributed execution using `dask.distributed`\n",
    "    * Workflows for the distributed scheduler might be different than those presented below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "* If you don't have a big data problem, don't use a big data tool\n",
    "* Many of the below examples could easily be handled in-memory with some better choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"http://dask.pydata.org/en/latest/_images/collections-schedulers.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subset of numpy ndarray interface using blocked algorithms\n",
    "* Dask array complements large on-disk array stores like HDF5, NetCDF, and BColz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVG(\"http://dask.pydata.org/en/latest/_images/dask-array-black-text.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Arithmetic and scalar mathematics, `+, *, exp, log, ...`\n",
    "* Reductions along axes, `sum(), mean(), std(), sum(axis=0), ...`\n",
    "* Tensor contractions / dot products / matrix multiply, `tensordot`\n",
    "* Axis reordering / transpose, `transpose`\n",
    "* Slicing, `x[:100, 500:100:-2]`\n",
    "* Fancy indexing along single axes with lists or numpy arrays, `x[:, [10, 1, 5]]`\n",
    "* The array protocol `__array__`\n",
    "* Some linear algebra `svd, qr, solve, solve_triangular, lstsq`\n",
    "\n",
    "[Full API Documentation](http://dask.pydata.org/en/latest/array-api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a dask array using `da.arange` analogous to `np.arange`\n",
    "* The idea of the `chunk` is important and has performance implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = da.arange(25, chunks=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask operates on a delayed computation model\n",
    "* It builds up an expression of the computation in chunks\n",
    "* Creates a **Task Graph** that you can explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can execute the graph by using **`compute`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As an example of the `__array__` protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `da.array`, `x` that has 25 chunks and is made up of the first 2500 numbers and take the square root of all of the elements. Print the last element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [ Solution Here ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load solutions/dask_root.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling Backends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can control the scheduler backend that is used by `compute`\n",
    "* These choices can be important in a few situations\n",
    "  * Debugging\n",
    "  * Fast tasks\n",
    "  * Cross-task communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronous Queue Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `dask.get` is an alias for the synchronous backend. Useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.compute(get=dask.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threaded Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `dask.threaded.get` is the default\n",
    "* Uses a thread pool backend\n",
    "* A thread is the smallest unit of work that an OS can schedule\n",
    "  * Threads are \"lightweight\"\n",
    "* They execute within the same process and thus share the same memory and file resources ([everything is a file](https://en.wikipedia.org/wiki/Everything_is_a_file) in unix)\n",
    "* Limitations\n",
    "  * Limited by the Global Interpreter Lock (GIL)\n",
    "    * A GIL means that only one thread can execute at the same time\n",
    "  * Pure python functions likely won't show a speed-up (with a few exceptions)\n",
    "    * C code can release the GIL\n",
    "    * I/O tasks are not blocked by the GIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.compute(get=dask.threaded.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default, dask will use as many threads as there are logical processors on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Backend that uses multiprocessing\n",
    "* Uses a process pool backend\n",
    "  * On unix-like system this is a system call to `fork`\n",
    "  * Calling `fork` creates a new child process which is a *copy*(-on-write) of the parent process\n",
    "  * Owns its own resources. This is \"heavy\"\n",
    "* Limitations\n",
    "  * Relies on serializing objects for the workers (slow and error prone)\n",
    "  * Workers must communicate through parent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.multiprocessing\n",
    "\n",
    "y.compute(get=dask.multiprocessing.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is part of the `dask.distributed` library\n",
    "* Distributes work over the network across machines using web sockets and an asychronous web framework for Python (tornado)\n",
    "  * Some recent additions make this work for, e.g., distributed DataFrames\n",
    "* We're not going to cover the distributed scheduler\n",
    "  * Matt and Jim gave a [nice talk](https://www.youtube.com/watch?v=PAGjm4BMKlk) at SciPy 2016 on distributed computing with dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a billion number array of 32-bit floats on disk using HDF5\n",
    "* HDF5 is an implementation of the Hierarchical Data Format common in scientific applications\n",
    "  * Multiple data formats (tables, nd-arrays, raster images)\n",
    "  * Fast lookups via B-tree indices (like SQL)\n",
    "  * Filesystem-like data format\n",
    "  * Support for meta-information\n",
    "* **Warning**, the result of this operation is 4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def random_array():\n",
    "    if os.path.exists(os.path.join('data', 'random.hdf5')):\n",
    "        return\n",
    "\n",
    "    print(\"Create random data for array exercise\")\n",
    "\n",
    "    with h5py.File(os.path.join('data', 'random.hdf5')) as f:\n",
    "        dset = f.create_dataset('/x', shape=(1000000000,), dtype='f4')\n",
    "        for i in range(0, 1000000000, 1000000):\n",
    "            dset[i: i + 1000000] = np.random.exponential(size=1000000)\n",
    "\n",
    "random_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocked Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask works on arrays by executing blocked algorithms on chunks of data\n",
    "* For example, consider taking the mean of a billion numbers. We might instead break up the array into 1,000 chunks, each of size 1,000,000, take the sum of each chunk, and then take the sum of the intermediate sums and divide this by the total number of observations.\n",
    "* The result (one sum on one billion numbers) is obtained from calculating many smaller results (one thousand sums on one million numbers each, followed by another sum of a thousand numbers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "f = h5py.File(os.path.join('data', 'random.hdf5'))\n",
    "dset = f['/x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we were to implement this ourselves it might look like this\n",
    "1. Computing the sum of each 1,000,000 sized chunk of the array\n",
    "2. Computing the sum of the 1,000 intermediate sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums = []\n",
    "for i in range(0, 1000000000, 1000000):\n",
    "    chunk = dset[i: i + 1000000]\n",
    "    sums.append(chunk.sum())\n",
    "\n",
    "total = np.sum(sums)\n",
    "print(total / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask does this for you and uses the backend scheduler to do so in parallel\n",
    "* Create a dask array from an array-like structure (any object that implements numpy-like slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = da.from_array(dset, chunks=(1000000, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x looks and behaves much like a numpy array\n",
    "  * Arithmetic, slicing, reductions\n",
    "* Use tab-completion to look at the methods of `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[:10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `dask.array.random.normal` to create a 20,000 x 20,000 array $X ~ \\sim N(10, .1)$ with `chunks` set to `(1000, 1000)`\n",
    "\n",
    "Take the mean of every 100th element along axis 0.\n",
    "\n",
    "*Hint*: Recall you can slice with the following syntax [start:end:step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load solutions/dask_array.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance vs. NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your performance may vary. If you attempt the NumPy version then please ensure that you have more than 4GB of main memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "y = x[::100].mean(axis=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster and needs only MB of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000), chunks=(1000, 1000))\n",
    "y = x[::100].mean(axis=0)\n",
    "y.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask implements a few linear algebra functions that are paraellizable\n",
    "* `da.linalg.qr`\n",
    "* `da.linalg.cholesky`\n",
    "* `da.linalg.svd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In mathematics, a bags is multiset\n",
    "* In dask, we can think of them as parallel lists for semi-structured data\n",
    "  * Nested, variable length, heterogenously typed, etc.\n",
    "  * E.g., JSON blobs or text data\n",
    "* Anything that can be represented as a large collection of generic Python objects \n",
    "* Mainly used for cleaning and processing\n",
    "  * I.e., usually the first step in a workflow \n",
    "* Bag implements a number of useful methods for operation on sequences like `map`, `filter`, `fold`, `frequencies` and `groupby`\n",
    "* Streaming computation on top of generators\n",
    "* Bags use the multiprocessing backend by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Aside\n",
    "\n",
    "* What are generators in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func(n=10):\n",
    "    for i in range(n):\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in func():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food Inspection Data\n",
    "\n",
    "* http://opendatadc.org/dataset/restaurant-inspection-data\n",
    "* Some processing in `data/clean_inspections.py` results in `data/inspections.zip`\n",
    "* This archive contains several gzipped files\n",
    "* Each file contains health inspection data for a month and a year\n",
    "* Each line in each file is a JSON object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First unzip the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "inspections_dir = os.path.join('data', 'inspections')\n",
    "with zipfile.ZipFile(os.path.join('data', 'inspections.zip')) as zf:\n",
    "    zf.extractall(path=inspections_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag = db.read_text(os.path.join(inspections_dir, 'inspections*.json.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `take` to grab a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is just unprocessed text\n",
    "* Using map to process the lines in the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json  # ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "js = bag.map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "js.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the inspection types?\n",
    "* We can grab a field using `pluck` and return the `distinct` elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visits = js.pluck('Inspection Type').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visits.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visits.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How often do we see these inspection types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = js.pluck('Inspection Type').frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Easier to read if sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "sorted(counts, key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `filter` and `take` the first 5 records with a `Complaint` inspection type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution Here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/health_complaints.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy / FoldBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GroupBy collects items in your collection so that all items with the same value under some function are collected together into a key-value pair.\n",
    "* This requires a full on-disk shuffle and is *very* inefficient\n",
    "* You almost never want to do this in a real workflow if you can avoid it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = db.from_sequence(['Alice', 'Bob', 'Charlie', 'Dan', 'Edith', 'Frank'])\n",
    "b.groupby(len).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Group by evens and odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = db.from_sequence(list(range(10)))\n",
    "b.groupby(lambda x: x % 2).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by evens and odds and take the largest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.groupby(lambda x: x % 2).map(lambda k, v: (k, max(v))).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FoldBy, while harder to grok, is much more efficient\n",
    "* This does a streaming combined groupby and reduction\n",
    "* Familiar to Spark users as the `combineByKey` method on `RDD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using foldby you provide\n",
    "\n",
    "1. A key function on which to group elements\n",
    "2. A binary operator such as you would pass to reduce that you use to perform reduction per each group\n",
    "3. A combine binary operator that can combine the results of two reduce calls on different parts of your dataset\n",
    "\n",
    "Your reduction must be associative. That is, the order in which the reduction takes place does not matter. \n",
    "\n",
    "It will happen in parallel in each of the partitions of your dataset. Then all of these intermediate results will be combined by the combine binary operator.\n",
    "\n",
    "Taking a step back, this is just what we saw in `sum` above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `functools.reduce` works like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(acc, y):\n",
    "    print(acc)\n",
    "    print(y)\n",
    "    print()\n",
    "    return acc + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "functools.reduce(func, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b.foldby?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.foldby(lambda x: x % 2, binop=max, combine=max).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the restaurant inspection data above, let's find the worst place to eat in DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "js.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many inpections per restuarant are there?\n",
    "  * These records all appear to be restaurants that have at least one reported violation not each visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = js.foldby(key='Name',\n",
    "                   binop=lambda total, x: total + 1,\n",
    "                   initial=0,\n",
    "                   combine=operator.add,            \n",
    "                   combine_initial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    result = counts.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(result, key=operator.itemgetter(1), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Of course, we could have computed this using `frequencies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = js.pluck('Name').frequencies()\n",
    "\n",
    "sorted(result, key=operator.itemgetter(1), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* In reality, we want to know the worst locations. Re-do the above by Name and Address. Use the `FoldBy` notation rather than frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We may be much more concerned with *Critical Violations*. Compute the number of critical violations for each restaurant name.\n",
    "* Decompose the problem in to pieces\n",
    "  * First, create a reduce function that computes the total for each inspection for each business name\n",
    "  * Change the above example to accumulate the total amount instead of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution Here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load solutions/bag_foldby.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are the types of violations that are caught?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "js.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use the `reduction` method of the `Bag` to compute this\n",
    "* `reduction` works first within each partition and then across each partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we want to pull ou the `Violations` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "violations = js.pluck('Violations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we have a Bag of lists, each of which contains the violations for an inspection\n",
    "* To reduce within the partitions, we need to iterate over the partition and within each list of violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_violations(inspections):\n",
    "    return {x['Violation Category'] for inspection in inspections for x in inspection}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To reduce of the partitions, we need to use a set union reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate(sets):\n",
    "    return functools.reduce(set.union, sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "js.pluck('Violations').reduction(get_violations, aggregate).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* What are the most frequent violations for each restaurant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/violation_counts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "source: http://britains-diet.labs.theodi.org/data/\n",
    "\n",
    "info: https://www.gov.uk/government/statistics/family-food-open-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile('data/nfs.zip')\n",
    "zf.extractall(path='data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* subset of the pandas API\n",
    "* Good for analyzing heterogenously typed tabular data arranged along an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\", width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trivially parallelizable operations (fast)**:\n",
    "\n",
    "  * Elementwise operations: `df.x + df.y, df * df`\n",
    "  * Row-wise selections: `df[df.x > 0]`\n",
    "  * Loc: `df.loc[4.0:10.5]`\n",
    "  * Common aggregations: `df.x.max(), df.max()`\n",
    "  * Is in: `df[df.x.isin([1, 2, 3])]`\n",
    "  * Datetime/string accessors: `df.timestamp.dt.month`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleverly parallelizable operations (fast)**:\n",
    "\n",
    "  * groupby-aggregate (with common aggregations): `df.groupby(df.x).y.max(), df.groupby('x').max()`\n",
    "  * value_counts: `df.x.value_counts()`\n",
    "  * Drop duplicates: `df.x.drop_duplicates()`\n",
    "  * Join on index: `dd.merge(df1, df2, left_index=True, right_index=True)`\n",
    "  * Join with Pandas DataFrames: `dd.merge(df1, df2, on='id')`\n",
    "  * Elementwise operations with different partitions: `df1.x + df2.y`\n",
    "  * Datetime resampling: `df.resample(...)`\n",
    "  * Rolling averages: `df.rolling(...)`\n",
    "  * Pearson Correlations: `df[['col1', 'col2']].corr()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operations requiring a shuffle (slow-ish, unless on index)**\n",
    "\n",
    "  * Set index: `df.set_index(df.x)`\n",
    "  * groupby-apply (with anything): `df.groupby(df.x).apply(myfunc)`\n",
    "  * Join not on the index: `dd.merge(df1, df2, on='name')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Full DataFrame API](http://dask.pydata.org/en/latest/dataframe-api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"data/nfs/NFS*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `DataFrame.head` is one operation that is not lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default the data is partitioned by the file\n",
    "* In our case, this is good. The files have a natural partition\n",
    "* When this is not the case, you must do a disk-based shuffle which is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.known_divisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are going to set the partition explicitly to `styr` to make some operations more performant\n",
    "* Partitions are denoted by the left-side of the bins for the partitions.\n",
    "* The final value is assumed to be the inclusive right-side for the last bin.\n",
    "\n",
    "\n",
    "So\n",
    "\n",
    "```\n",
    "[1974, 1975, 1976]\n",
    "```\n",
    "\n",
    "Would be 2 partitions. The first contains 1974. The second contains 1975 and 1976. To get three partitions, one for the final observation, duplicate it.\n",
    "\n",
    "```\n",
    "[1974, 1975, 1976, 1976]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitions = list(range(1974, 2001)) + [2000]\n",
    "\n",
    "df = df.set_partition('styr', divisions=partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.known_divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.divisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nothing yet is loaded in to memory\n",
    "* Meta-information from pandas is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In addition to the (supported) pandas DataFrame API, dask provides a few more convenient methods\n",
    "    * `DataFrame.categorize`\n",
    "    * `DataFrame.map_partitions`\n",
    "    * `DataFrame.get_partition`\n",
    "    * `DataFrame.repartition`\n",
    "    * `DataFrame.set_partition`\n",
    "    * `DataFrame.to_{bag|castra}`\n",
    "    * `DataFrame.visualize`\n",
    "    \n",
    "* A few methods have a slightly different API\n",
    "\n",
    "    * `DataFrame.apply`\n",
    "    * `GroupBy.apply`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2000 = df.get_partition(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(df2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What food group was consumed the most times in 2000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp = df2000.groupby('minfd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = grp.apply(len, meta='size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Turn it into a Series first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minfd = size.compute().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(minfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the pre-processed mapping across food grouping variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food_mapping = pd.read_csv(\"data/nfs/food_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas provides the efficient `isin` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "food_mapping.ix[food_mapping.minfd.isin([minfd])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* What was the most consumed food group in 1974?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load solutions/nfs_most_purchased.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Map partitions does what you might expect\n",
    "* Maps a function across partitions\n",
    "* Let's calculate the most frequently purchase food group for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_frequent_food(partition):\n",
    "    # partition is a pandas.DataFrame\n",
    "    grpr = partition.groupby('minfd')\n",
    "    size = grpr.size()\n",
    "    minfd = size.idxmax()\n",
    "    idx = food_mapping.minfd.isin([minfd])\n",
    "    description = food_mapping.ix[idx].minfddesc.iloc[0]\n",
    "    year = int(partition.index[0])\n",
    "    return year, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnfd_year = df.map_partitions(most_frequent_food, \n",
    "                              meta={'year': int,\n",
    "                                    'description': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnfd_year.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Within each year, group by household `minfd` and calculate daily per capita consumption of each food group. Hint, you want to use `map_partitions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution Here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/average_consumption.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
